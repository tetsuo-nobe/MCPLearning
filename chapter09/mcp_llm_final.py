#!/usr/bin/env python3
"""
LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå®Œå…¨ç‰ˆ V3 - å…ƒã®ã‚³ãƒ¼ãƒ‰ä¿æŒç‰ˆï¼‰
Step 1-3ã®æˆæœã‚’çµ±åˆã—ãŸå®Ÿç”¨çš„ãªå¯¾è©±å‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

â€»æ¥ç¶šéƒ¨åˆ†ã®ã¿ä¿®æ­£ã€ãã®ä»–ã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã®å‹•ä½œã‚’å®Œå…¨ã«ä¿æŒ
"""

import asyncio
import os
import sys

# Windowsç’°å¢ƒã§ã®Unicodeå¯¾å¿œ
if sys.platform == "win32":
    os.environ["PYTHONIOENCODING"] = "utf-8"
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv
#from openai import AsyncOpenAI
import boto3
from fastmcp import Client
from fastmcp.client.transports import StdioTransport

# Step 1-3ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from mcp_llm_step1 import ToolCollector
from mcp_llm_step2 import LLMIntegrationPrep

load_dotenv()

class CompleteLLMClient:
    """å®Œå…¨ãªLLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ä¿æŒç‰ˆï¼‰"""
    
    def __init__(self):
        # Step 1-3ã®ã‚¯ãƒ©ã‚¹ã‚’æ´»ç”¨
        self.collector = ToolCollector()
        self.prep = LLMIntegrationPrep()
        #self.llm = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.llm = boto3.client(service_name="bedrock-runtime", region_name='us-east-1')
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç†
        self.clients = {}
        
        # ä¼šè©±å±¥æ­´ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        self.conversation_history = []
        self.context = {
            "session_start": datetime.now(),
            "tool_calls": 0,
            "errors": 0
        }
        
    async def initialize(self):
        """åˆæœŸåŒ–å‡¦ç†"""
        print("[èµ·å‹•] LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’èµ·å‹•ä¸­...", flush=True)
        
        # Step 1: ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’åé›†
        await self.collector.collect_all_tools()
        
        # MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æ¥ç¶šï¼ˆStdioTransportå¯¾å¿œï¼‰
        for server_name, server_info in self.collector.servers.items():
            try:
                command = server_info["path"][0]
                args = server_info["path"][1:]
                transport = StdioTransport(command=command, args=args)
                client = Client(transport)
                await client.__aenter__()
                self.clients[server_name] = client
            except Exception as e:
                print(f"  [WARNING] {server_name}ã¸ã®æ¥ç¶šå¤±æ•—: {e}")
        
        print("[å®Œäº†] åˆæœŸåŒ–å®Œäº†\n", flush=True)
        self._show_available_tools()
    
    def _show_available_tools(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’è¡¨ç¤º"""
        total_tools = sum(len(tools) for tools in self.collector.tools_schema.values())
        print(f"[ãƒ„ãƒ¼ãƒ«] åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«: {total_tools}å€‹")
        for server_name, tools in self.collector.tools_schema.items():
            print(f"  - {server_name}: {len(tools)}å€‹ã®ãƒ„ãƒ¼ãƒ«")
        print()
    
    async def _analyze_query(self, query: str) -> Dict:
        """ã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã€ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã®å¿…è¦æ€§ã¨å¯¾å¿œã‚’æ±ºå®š"""
        tools_desc = self.prep.prepare_tools_for_llm(self.collector.tools_schema)
        
        # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€å¤§5ä»¶ï¼‰
        recent_history = ""
        if self.conversation_history:
            recent_messages = self.conversation_history[-5:]
            history_lines = []
            for msg in recent_messages:
                role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
                history_lines.append(f"{role}: {msg['content']}")
            recent_history = "\n".join(history_lines)
        
        prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æã—ã€é©åˆ‡ãªå¯¾å¿œã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚

## ã“ã‚Œã¾ã§ã®ä¼šè©±
{recent_history if recent_history else "ï¼ˆæ–°ã—ã„ä¼šè©±ï¼‰"}

## ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{query}

## åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«
{tools_desc}

## åˆ¤å®šåŸºæº–
- è¨ˆç®—ã€ãƒ‡ãƒ¼ã‚¿å–å¾—ã€å¤–éƒ¨æƒ…å ±ã®å‚ç…§ã€ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡ŒãŒå¿…è¦ â†’ needs_tool: true
- ä¸€èˆ¬çš„ãªçŸ¥è­˜ã€èª¬æ˜ã€ä¼šè©±ã€æ„è¦‹ã€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ç­”ãˆã‚‰ã‚Œã‚‹ â†’ needs_tool: false
- é‡è¦ï¼šã“ã‚Œã¾ã§ã®ä¼šè©±ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„

## å¿œç­”å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å¿…ãšå¿œç­”ã—ã¦ãã ã•ã„ï¼ˆJSONã®ã¿ã€èª¬æ˜æ–‡ã¯ä¸è¦ï¼‰ï¼š

needs_tool=trueã®å ´åˆ:
{{
  "needs_tool": true,
  "server": "ã‚µãƒ¼ãƒãƒ¼åã®ã¿ï¼ˆä¾‹: calculatorï¼‰",
  "tool": "ãƒ„ãƒ¼ãƒ«åã®ã¿ï¼ˆä¾‹: addï¼‰â€»ã‚µãƒ¼ãƒãƒ¼åã¯å«ã‚ãªã„",
  "arguments": {{ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿}},
  "reasoning": "ãªãœã“ã®ãƒ„ãƒ¼ãƒ«ã‚’é¸ã‚“ã ã‹"
}}

needs_tool=falseã®å ´åˆ:
{{
  "needs_tool": false,
  "reasoning": "ãªãœãƒ„ãƒ¼ãƒ«ãŒä¸è¦ã‹",
  "response": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ç›´æ¥å›ç­”"
}}

## é‡è¦ãªæ³¨æ„
- ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã¯ "ã‚µãƒ¼ãƒãƒ¼å.ãƒ„ãƒ¼ãƒ«å" ã®å½¢å¼ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ãŒ
- JSONã§ã¯ server ã¨ tool ã‚’åˆ¥ã€…ã«æŒ‡å®šã—ã¦ãã ã•ã„
- ä¾‹: "calculator.add" â†’ server: "calculator", tool: "add"
- ä¾‹: "weather.get_weather" â†’ server: "weather", tool: "get_weather"
"""
        
        # response = await self.llm.chat.completions.create(
        #     model="gpt-4o-mini",
        #     messages=[
        #         {"role": "system", "content": "You are a helpful assistant that analyzes queries and determines appropriate actions. Always respond with valid JSON only."},
        #         {"role": "user", "content": prompt}
        #     ],
        #     temperature=0
        # )

        response = self.llm.converse(
            modelId='anthropic.claude-3-sonnet-20240229-v1:0',
            system= [{"text": "You are a helpful assistant that analyzes queries and determines appropriate actions. Always respond with valid JSON only."}],
            messages=[
                {'role': 'user',"content": [{"text": prompt}]}
            ],
            inferenceConfig= {"temperature": 0}
        )
        
        # ãƒ‡ãƒãƒƒã‚°: LLMã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
        raw_response = response['output']['message']['content'][0]['text']
        #raw_response = response.choices[0].message.content
        
        print(f"  [LLM] ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæœ€åˆã®300æ–‡å­—ï¼‰:", flush=True)
        print(f"  {raw_response[:300]}...", flush=True)
        
        try:
            return self.prep.validate_llm_response(raw_response)
        except Exception as e:
            print(f"  [ERROR] ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"  [INFO] å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹:")
            print(raw_response)
            raise
    
    async def process_query(self, query: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã‚’å‡¦ç†"""
        try:
            # ã‚¯ã‚¨ãƒªã‚’åˆ†æï¼ˆä¼šè©±å±¥æ­´ã‚’å‚ç…§ã—ã¤ã¤ï¼‰
            print("  [åˆ†æ] ã‚¯ã‚¨ãƒªã‚’åˆ†æä¸­...", flush=True)
            decision = await self._analyze_query(query)
            
            # åˆ†æå¾Œã«ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({"role": "user", "content": query})
            
            # åˆ¤æ–­ç†ç”±ã‚’è¡¨ç¤º
            if decision.get("reasoning"):
                print(f"  [åˆ¤æ–­] {decision['reasoning']}", flush=True)
            
            if decision.get("needs_tool", False):
                # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ‘ã‚¹
                print(f"  [é¸æŠ] ãƒ„ãƒ¼ãƒ«: {decision['server']}.{decision['tool']}", flush=True)
                print(f"     å¼•æ•°: {decision['arguments']}", flush=True)
                print(f"  [å®Ÿè¡Œ] å‡¦ç†ä¸­...", flush=True)
                
                result = await self._execute_tool(
                    decision['server'],
                    decision['tool'],
                    decision['arguments']
                )
                print(f"  [å®Œäº†] å®Ÿè¡Œå®Œäº†", flush=True)
                
                # çµæœã‚’è§£é‡ˆ
                print("  [è§£é‡ˆ] çµæœã‚’è§£é‡ˆä¸­...", flush=True)
                return await self._interpret_result(query, decision, result)
            else:
                # ç›´æ¥å¿œç­”ãƒ‘ã‚¹
                print("  [å¿œç­”] ç›´æ¥å¿œç­”ãƒ¢ãƒ¼ãƒ‰", flush=True)
                response = decision.get("response", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                self.conversation_history.append({"role": "assistant", "content": response})
                return response
                
        except Exception as e:
            self.context["errors"] += 1
            return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    async def _execute_tool(self, server: str, tool: str, arguments: Dict) -> Any:
        """MCPãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
        if server not in self.clients:
            raise ValueError(f"ã‚µãƒ¼ãƒãƒ¼ '{server}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        self.context["tool_calls"] += 1
        client = self.clients[server]
        result = await client.call_tool(tool, arguments)
        
        # çµæœã‚’é©åˆ‡ãªå½¢å¼ã§å–å¾—
        if hasattr(result, 'content'):
            if isinstance(result.content, list) and result.content:
                first = result.content[0]
                if hasattr(first, 'text'):
                    return first.text
        return str(result)
    
    async def _interpret_result(self, query: str, decision: Dict, result: Any) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†ã‹ã‚Šã‚„ã™ãè§£é‡ˆ"""
        interpretation_prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’ã‚‚ã¨ã«ã€ã‚ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query}
å®Ÿè¡Œã—ãŸãƒ„ãƒ¼ãƒ«: {decision['server']}.{decision['tool']}
ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæœ: {result}

## æŒ‡ç¤º
1. ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæœã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç†è§£ã—ã‚„ã™ã„ã‚ˆã†ã«èª¬æ˜ã—ã¦ãã ã•ã„
2. å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã®è§£é‡ˆã‚„èª¬æ˜ã‚’åŠ ãˆã¦ãã ã•ã„
3. ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å¯èƒ½ã§ã‚ã‚Œã°ãã®ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
4. çµæœãŒæœŸå¾…ã¨ç•°ãªã‚‹å ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„

## å›ç­”å½¢å¼
è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆJSONå½¢å¼ã¯ä¸è¦ï¼‰ã€‚
"""
        
        # response = await self.llm.chat.completions.create(
        #     model="gpt-4o-mini",
        #     messages=[
        #         {"role": "system", "content": "You are a helpful assistant that interprets tool results for users in a clear and friendly manner."},
        #         {"role": "user", "content": interpretation_prompt}
        #     ],
        #     temperature=0.3
        # )
        
        # interpreted_response = response.choices[0].message.content
        
        response = self.llm.converse(
            modelId='anthropic.claude-3-sonnet-20240229-v1:0',
            system= [{"text": "You are a helpful assistant that interprets tool results for users in a clear and friendly manner."}],
            messages=[
                {'role': 'user',"content": [{"text": interpretation_prompt}]}
            ],
            inferenceConfig= {"temperature": 0}
        )
        interpreted_response = response['output']['message']['content'][0]['text']
        self.conversation_history.append({"role": "assistant", "content": interpreted_response})
        
        return interpreted_response
    
    async def interactive_mode(self):
        """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã®å®Ÿè¡Œ"""
        print("\n" + "="*60)
        print("ğŸ¤– LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ V3 - å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
        print("="*60)
        print("è‡ªç„¶è¨€èªã§MCPãƒ„ãƒ¼ãƒ«ã‚’æ“ä½œã§ãã¾ã™ã€‚")
        print("ä½¿ç”¨ä¾‹: '10ã¨20ã‚’è¶³ã—ã¦', 'æ±äº¬ã®å¤©æ°—ã‚’æ•™ãˆã¦'")
        print("ç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰: help, status, history, quit")
        print("="*60 + "\n")
        
        while True:
            try:
                user_input = input("ğŸ’¬ ã‚ãªãŸ: ").strip()
                
                if not user_input:
                    continue
                
                # ç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ ãŠç–²ã‚Œã•ã¾ã§ã—ãŸï¼")
                    break
                elif user_input.lower() in ['help', '?']:
                    self._show_help()
                    continue
                elif user_input.lower() == 'status':
                    self._show_status()
                    continue
                elif user_input.lower() == 'history':
                    self._show_history()
                    continue
                elif user_input.lower() == 'tools':
                    self._show_available_tools()
                    continue
                
                # é€šå¸¸ã®ã‚¯ã‚¨ãƒªå‡¦ç†
                print("\nğŸ” å‡¦ç†ä¸­...")
                response = await self.process_query(user_input)
                print(f"\nğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {response}\n")
                
            except KeyboardInterrupt:
                print("\n\n[STOP] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"\n[ERROR] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n")
    
    def _show_help(self):
        """ãƒ˜ãƒ«ãƒ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º"""
        print("\n" + "="*50)
        print("ğŸ“– ãƒ˜ãƒ«ãƒ—")
        print("="*50)
        print("â€¢ è‡ªç„¶è¨€èªã§MCPãƒ„ãƒ¼ãƒ«ã‚’æ“ä½œã§ãã¾ã™")
        print("â€¢ ä¾‹: '100ã¨250ã‚’è¶³ã—ã¦', 'æ±äº¬ã®å¤©æ°—ã‚’æ•™ãˆã¦'")
        print("\nç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰:")
        print("  help - ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º")
        print("  status - ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º")
        print("  history - ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤º")
        print("  tools - åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’è¡¨ç¤º")
        print("  quit - ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†")
        print("="*50 + "\n")
    
    def _show_status(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º"""
        duration = datetime.now() - self.context["session_start"]
        print("\n" + "="*50)
        print("ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±")
        print("="*50)
        print(f"èµ·å‹•æ™‚é–“: {self.context['session_start'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"çµŒéæ™‚é–“: {str(duration).split('.')[0]}")
        print(f"æ¥ç¶šã‚µãƒ¼ãƒãƒ¼æ•°: {len(self.clients)}")
        print(f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå›æ•°: {self.context['tool_calls']}")
        print(f"ã‚¨ãƒ©ãƒ¼å›æ•°: {self.context['errors']}")
        print("="*50 + "\n")
    
    def _show_history(self):
        """ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤º"""
        if not self.conversation_history:
            print("\nğŸ“‹ ä¼šè©±å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“\n")
            return
        
        print("\n" + "="*50)
        print(f"ğŸ“‹ ä¼šè©±å±¥æ­´ï¼ˆæœ€æ–°{min(len(self.conversation_history), 10)}ä»¶ï¼‰")
        print("="*50)
        
        for i, msg in enumerate(self.conversation_history[-10:], 1):
            role = "ã‚ãªãŸ" if msg["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
            content = msg["content"][:80] + ("..." if len(msg["content"]) > 80 else "")
            print(f"{i:2d}. {role}: {content}")
        
        print("="*50 + "\n")
    
    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        for client in self.clients.values():
            try:
                await client.__aexit__(None, None, None)
            except:
                pass

async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # APIã‚­ãƒ¼ã®ç¢ºèª
    # if not os.getenv("OPENAI_API_KEY"):
    #     print("[ERROR] ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    #     print("ä¾‹: set OPENAI_API_KEY=your_api_key_here")
    #     return
    
    client = CompleteLLMClient()
    
    try:
        # åˆæœŸåŒ–
        await client.initialize()
        
        # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰é–‹å§‹
        await client.interactive_mode()
        
    except KeyboardInterrupt:
        print("\n[STOP] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"[FATAL] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        await client.cleanup()
        print("[EXIT] ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")

if __name__ == "__main__":
    asyncio.run(main())
